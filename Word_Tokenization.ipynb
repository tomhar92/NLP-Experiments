{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyywF+eZFh/UWGD5WcKbfv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomhar92/NLP-Experiments/blob/master/Word_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcW2reE6f_b0",
        "colab_type": "text"
      },
      "source": [
        "# Word Tokenization\n",
        "\n",
        "The task of word tokenization is all about segmenting running text into words. These tokens are very useful for finding patterns in the text. They are also considered to be a base step for stemming and lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhc8PZJr_G75",
        "colab_type": "text"
      },
      "source": [
        "# Byte Pair Encoding\n",
        "\n",
        "Instead of tokenizing words, we can use our text to automatically define what size our tokens should be. This is useful to breakdown words into sub-words because our algorithms will know how to handle unknown words. \n",
        "\n",
        "At each step of the algorithm we count the number of symbol pairs, and replace it with the new merged symbol. We continue to count and merge, creating new longer and longer character strings, until we have done K merges. The resulting symbol set will consist of the original set of characters plus k new symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvzYjJ1XMcIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "994ff26c-02b4-42d9-d158-347a744999b0"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=3d2ef481a55a7a8dbba742d3480242a96d574d41568a3f39af0feb4bf2313157\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y90KUS7Z9AtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia\n",
        "import re\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFppqn3iCkSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tokens(words):\n",
        "  word_tokens = {}\n",
        "  for word in words:\n",
        "    new_word = \"\"\n",
        "    for char in re.findall(\"[a-zA-Z]\", word):\n",
        "      if char.isupper():\n",
        "        new_word = new_word + char.lower();\n",
        "      else:\n",
        "        new_word = new_word + char;\n",
        "    if len(new_word) > 0:\n",
        "      new_word = new_word + \"_\";  \n",
        "    if new_word in word_tokens:\n",
        "      word_tokens[new_word] = word_tokens[new_word] + 1;\n",
        "    else:\n",
        "      word_tokens[new_word] = 1;   \n",
        "  return word_tokens;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKLF8BPeBLNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairs_from_page(text_content):\n",
        "  words = text_content.split(\" \");\n",
        "  word_tokens = get_word_tokens(words);\n",
        "  common_pairs = {}\n",
        "  for key in word_tokens:\n",
        "    symbols = list(key);\n",
        "    for i in range(0, len(symbols) - 1):\n",
        "      if symbols[i]+symbols[i+1] in common_pairs:\n",
        "        common_pairs[symbols[i]+symbols[i+1]] += 1\n",
        "      else:\n",
        "        common_pairs[symbols[i]+symbols[i+1]] = 1\n",
        "  return common_pairs;       \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u938M0bNsAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "5e014adb-5f01-4563-9da8-71cc05628b48"
      },
      "source": [
        "print(\"Welcome to Byte Pair Generator!\")\n",
        "number_of_pairs = int(input(\"Please enter the required number of Pairs\"))\n",
        "number_of_wiki_pages = int(input(\"Please enter the number of Wiki pages you would like to scan\"))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to Byte Pair Generator!\n",
            "Please enter the required number of Pairs10\n",
            "Please enter the number of Wiki pages you would like to scan10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATYodUn7ASB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "e683e1b7-d8a2-444a-9d15-1076d97c9440"
      },
      "source": [
        "common_pairs = {}\n",
        "page_counter = 1;\n",
        "while page_counter <= number_of_wiki_pages:\n",
        "  page_title = wikipedia.random(pages = 1);\n",
        "  try:  \n",
        "    text_content = wikipedia.WikipediaPage(title=page_title).content;\n",
        "  except wikipedia.exceptions.DisambiguationError as e:\n",
        "    page_title = e.options[0];\n",
        "    text_content = wikipedia.WikipediaPage(title=page_title).content;\n",
        "  print(\"Page Number \"+str(page_counter)+\" is: \"+page_title);\n",
        "  pairs_from_page = get_pairs_from_page(text_content);\n",
        "  for pair in pairs_from_page.items():\n",
        "    if pair[0] in common_pairs:\n",
        "      common_pairs[pair[0]] += pair[1];\n",
        "    else:\n",
        "      common_pairs[pair[0]] = pair[1];\n",
        "  page_counter += 1;\n",
        "print(\"The \"+str(number_of_pairs)+ \" most common pairs are:\")\n",
        "sorted_pairs = sorted(common_pairs.items(), key=lambda item: item[1], reverse=True)\n",
        "result_chars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'];\n",
        "for i in range(number_of_pairs):\n",
        "    print(sorted_pairs[i]);\n",
        "    result_chars.append(sorted_pairs[i][0]);\n",
        "print(\"The resulted vocabulary of characters:\")\n",
        "print(result_chars)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Page Number 1 is: Downtown LaPorte Historic District\n",
            "Page Number 2 is: Maud Mary Brindley\n",
            "Page Number 3 is: Sanju Yadav\n",
            "Page Number 4 is: Lenny Walls\n",
            "Page Number 5 is: Lene MykjÃ¥land\n",
            "Page Number 6 is: Jordan Brown (footballer, born 1991)\n",
            "Page Number 7 is: Zeta Phi Eta\n",
            "Page Number 8 is: Tsuruga Nursing University\n",
            "Page Number 9 is: Westminster School District\n",
            "Page Number 10 is: Lime kilns, Oeiras, Portugal\n",
            "The 10 most common pairs are:\n",
            "('e_', 258)\n",
            "('s_', 237)\n",
            "('er', 232)\n",
            "('in', 227)\n",
            "('d_', 206)\n",
            "('n_', 186)\n",
            "('on', 164)\n",
            "('re', 163)\n",
            "('ed', 150)\n",
            "('te', 145)\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'e_', 's_', 'er', 'in', 'd_', 'n_', 'on', 're', 'ed', 'te']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwv02S2mFzXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}