{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Word Tokenization.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOUqYxZgSQnp4LOqhKHaZUT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tomhar92/NLP-Experiments/blob/master/Word_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CcW2reE6f_b0",
        "colab_type": "text"
      },
      "source": [
        "# Word Tokenization\n",
        "\n",
        "The task of word tokenization is all about segmenting running text into words. These tokens are very useful for finding patterns in the text. They are also considered to be a base step for stemming and lemmatization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhc8PZJr_G75",
        "colab_type": "text"
      },
      "source": [
        "# Byte Pair Encoding\n",
        "\n",
        "Instead of tokenizing words, we can use our text to automatically define what size our tokens should be. This is useful to breakdown words into sub-words because our algorithms will know how to handle unknown words. \n",
        "\n",
        "At each step of the algorithm we count the number of symbol pairs, and replace it with the new merged symbol. We continue to count and merge, creating new longer and longer character strings, until we have done K merges. The resulting symbol set will consist of the original set of characters plus k new symbols."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RvzYjJ1XMcIv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "994ff26c-02b4-42d9-d158-347a744999b0"
      },
      "source": [
        "!pip install wikipedia"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wikipedia\n",
            "  Downloading https://files.pythonhosted.org/packages/67/35/25e68fbc99e672127cc6fbb14b8ec1ba3dfef035bf1e4c90f78f24a80b7d/wikipedia-1.4.0.tar.gz\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (4.6.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wikipedia) (2.21.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2019.11.28)\n",
            "Building wheels for collected packages: wikipedia\n",
            "  Building wheel for wikipedia (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wikipedia: filename=wikipedia-1.4.0-cp36-none-any.whl size=11686 sha256=3d2ef481a55a7a8dbba742d3480242a96d574d41568a3f39af0feb4bf2313157\n",
            "  Stored in directory: /root/.cache/pip/wheels/87/2a/18/4e471fd96d12114d16fe4a446d00c3b38fb9efcb744bd31f4a\n",
            "Successfully built wikipedia\n",
            "Installing collected packages: wikipedia\n",
            "Successfully installed wikipedia-1.4.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y90KUS7Z9AtQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import wikipedia\n",
        "import re\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gFppqn3iCkSW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_word_tokens(words):\n",
        "  word_tokens = {}\n",
        "  for word in words:\n",
        "    new_word = \"\"\n",
        "    for char in re.findall(\"[a-zA-Z]\", word):\n",
        "      if char.isupper():\n",
        "        new_word = new_word + char.lower();\n",
        "      else:\n",
        "        new_word = new_word + char;\n",
        "    if len(new_word) > 0:\n",
        "      new_word = new_word + \"_\";  \n",
        "    if new_word in word_tokens:\n",
        "      word_tokens[new_word] = word_tokens[new_word] + 1;\n",
        "    else:\n",
        "      word_tokens[new_word] = 1;   \n",
        "  return word_tokens;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKLF8BPeBLNQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_pairs_from_page(text_content, length_of_pairs):\n",
        "  words = text_content.split(\" \");\n",
        "  word_tokens = get_word_tokens(words);\n",
        "  common_pairs = {}\n",
        "  for key in word_tokens:\n",
        "    symbols = list(key);\n",
        "    for i in range(0, len(symbols) - length_of_pairs + 1):\n",
        "      pair = '';\n",
        "      for j in range(length_of_pairs):\n",
        "        pair = pair + symbols[i+j];\n",
        "      if pair in common_pairs:\n",
        "        common_pairs[pair] += 1\n",
        "      else:\n",
        "        common_pairs[pair] = 1\n",
        "  return common_pairs; \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o5asFtP9rhM_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_page_contents(number_of_wiki_pages):\n",
        "  page_contents = [];\n",
        "  while len(page_contents) < number_of_wiki_pages:\n",
        "    page_title = wikipedia.random(pages = 1);\n",
        "    try:  \n",
        "      text_content = wikipedia.WikipediaPage(title=page_title).content;\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "      try:\n",
        "        page_title = e.options[0];\n",
        "        text_content = wikipedia.WikipediaPage(title=page_title).content;\n",
        "      except wikipedia.exceptions.WikipediaException as e:\n",
        "        continue;\n",
        "    print(\"Page Number \"+str(len(page_contents) + 1)+\" is: \"+page_title);\n",
        "    page_contents.append(text_content);\n",
        "  return page_contents;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u938M0bNsAF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "607b9cc6-b681-4e73-d107-2336cab7f0a9"
      },
      "source": [
        "print(\"Welcome to Byte Pair Generator!\")\n",
        "number_of_pairs = int(input(\"Please enter the required number of Pairs\"))\n",
        "number_of_wiki_pages = int(input(\"Please enter the number of Wiki pages you would like to scan\"))"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Welcome to Byte Pair Generator!\n",
            "Please enter the required number of Pairs1000\n",
            "Please enter the number of Wiki pages you would like to scan10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATYodUn7ASB1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        },
        "outputId": "260536d8-326b-4a04-da14-eacf7de7b628"
      },
      "source": [
        "common_pairs = {}\n",
        "pair_length = 1;\n",
        "page_contents = get_page_contents(number_of_wiki_pages);\n",
        "while len(common_pairs) < number_of_pairs:\n",
        "  pair_length = pair_length + 1;\n",
        "  print(\"Pair Length: \"+str(pair_length))\n",
        "  for i in range(len(page_contents)):\n",
        "    pairs_from_page = get_pairs_from_page(page_contents[i], pair_length);\n",
        "    for pair in pairs_from_page.items():\n",
        "      if pair[0] in common_pairs:\n",
        "        common_pairs[pair[0]] += pair[1];\n",
        "      else:\n",
        "        common_pairs[pair[0]] = pair[1];\n",
        "    print(\"Length of pairs: \"+str(len(common_pairs)))\n",
        "sorted_pairs = sorted(common_pairs.items(), key=lambda item: item[1], reverse=True)\n",
        "result_chars = ['a','b','c','d','e','f','g','h','i','j','k','l','m','n','o','p','q','r','s','t','u','v','w','x','y','z'];\n",
        "for i in range(number_of_pairs):\n",
        "  result_chars.append(sorted_pairs[i][0]);\n",
        "print(\"The resulted vocabulary of characters:\")\n",
        "print(result_chars)"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Page Number 1 is: Leszczanka, CheÅ‚m County\n",
            "Page Number 2 is: Willi Reimann\n",
            "Page Number 3 is: Jack Greenwell\n",
            "Page Number 4 is: Southern New England Telephone Company Building\n",
            "Page Number 5 is: Frederick Low (British politician)\n",
            "Page Number 6 is: HMS Foam\n",
            "Page Number 7 is: Sally Hepworth\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py:389: UserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.6/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Page Number 8 is: Shah Rukh Khan\n",
            "Page Number 9 is: Arrowhead (Charlottesville, Virginia)\n",
            "Page Number 10 is: Last Island, Louisiana\n",
            "Pair Length: 2\n",
            "Length of pairs: 124\n",
            "Length of pairs: 230\n",
            "Length of pairs: 346\n",
            "Length of pairs: 362\n",
            "Length of pairs: 370\n",
            "Length of pairs: 380\n",
            "Length of pairs: 395\n",
            "Length of pairs: 489\n",
            "Length of pairs: 489\n",
            "Length of pairs: 490\n",
            "Pair Length: 3\n",
            "Length of pairs: 638\n",
            "Length of pairs: 921\n",
            "Length of pairs: 1628\n",
            "Length of pairs: 1794\n",
            "Length of pairs: 1877\n",
            "Length of pairs: 1950\n",
            "Length of pairs: 2088\n",
            "Length of pairs: 3459\n",
            "Length of pairs: 3486\n",
            "Length of pairs: 3556\n",
            "The resulted vocabulary of characters:\n",
            "['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 's_', 'er', 'e_', 'in', 'd_', 're', 'an', 'ed', 'n_', 'on', 'es', 'ed_', 'te', 'en', 't_', 'r_', 'ar', 'al', 'st', 'ng', 'ti', 'y_', 'or', 'at', 'nt', 'co', 'is', 'ra', 'ing', 'ri', 'g_', 'ng_', 'se', 'de', 'a_', 'l_', 'er_', 'ha', 'th', 'le', 'he', 'me', 'ma', 've', 'it', 'li', 'io', 'ea', 'na', 'as', 'nd', 'ou', 'ne', 'la', 'ic', 'si', 'ro', 'ce', 'ta', 'es_', 'ch', 'll', 'sh', 'hi', 'el', 'h_', 'ion', 'ca', 'ent', 'ur', 'am', 'di', 'al_', 'om', 'ns', 'ni', 'tr', 'ac', 'pa', 'il', 'pe', 'rs', 'rt', 'on_', 'un', 'to', 'ec', 'ho', 'lo', 'ter', 'ai', 'mi', 'nc', 'ge', 'ia', 'ie', 'ol', 'ly', 'be', 'an_', 'tio', 'ct', 'ss', 'ee', 'ad', 'em', 'no', 'us', 'ly_', 'st_', 'ted', 'pr', 'ir', 'fo', 'ba', 'et', 'i_', 'po', 'fi', 'iv', 'rn', 'sa', 'os', 'the', 'ati', 'ot', 'nt_', 'vi', 'm_', 'o_', 'ate', 'we', 'est', 'ay', 'id', 'ab', 'so', 'con', 'her', 'su', 'ag', 'fe', 'mo', 'ul', 'bo', 'ut', 'oo', 'tu', 're_', 'rs_', 'ow', 'ap', 'ig', 'ts', 'com', 'ov', 'wo', 'ci', 'man', 'all', 'ns_', 'op', 'res', 'ers', 'ons', 'mp', 'sta', 'le_', 'fa', 'wa', 'and', 'nce', 'ka', 'ry', 'im', 'pi', 'ga', 'ld', 'sp', 'ver', 'men', 'ts_', 'ev', 'ist', 'nd_', 'for', 'k_', 'in_', 'ive', 'ere', 'wi', 'ep', 'gh', 'en_', 'ty', 'rm', 'rd', 'nal', 'as_', 'bl', 've_', 'red', 'han', 'per', 'ef', 'pl', 'gr', 'ste', 'ry_', 'ke', 'au', 'da', 'ove', 'th_', 'ar_', 'gi', 'ue', 'or_', 'nn', 'du', 'ica', 'cha', 'ide', 'iti', 'sc', 'ex', 'ona', 'der', 'tor', 'of', 'pp', 'ei', 'ff', 'tra', 'ces', 'ort', 'c_', 'gu', 'ck', 'ua', 'do', 'ear', 'ish', 'me_', 'tin', 'ain', 'ce_', 'tic', 'art', 'se_', 'f_', 'rr', 'bu', 'fr', 'ru', 'cr', 'tt', 'kh', 'ome', 'ne_', 'nte', 'od', 'ak', 'cl', 'ren', 'enc', 'ame', 'ran', 'sha', 'de_', 'lly', 'rit', 'eg', 'mb', 'uc', 'ern', 'pro', 'ine', 'rin', 'cal', 'int', 'te_', 'br', 'lu', 'p_', 'ye', 'oc', 'dr', 'min', 'ty_', 'ch_', 'eri', 'lis', 'ess', 'sin', 'ip', 'rg', 'w_', 'ill', 'nde', 'our', 'ree', 'eve', 'ant', 'tur', 'par', 'ect', 'ub', 'ki', 'rc', 'rl', 'mu', 'age', 'ge_', 'str', 'rat', 'out', 'll_', 'at_', 'end', 'eco', 'are', 'jo', 'wh', 'ls', 'cc', 'ud', 'um', 'oun', 'nti', 'anc', 'sed', 'lt', 'if', 'ah', 'mm', 'ana', 'lea', 'ite', 'sh_', 'ld_', 'ind', 'ay_', 'nin', 'ic_', 'ous', 'aa', 'up', 'ja', 'av', 'va', 'bi', 'is_', 'ric', 'eas', 'ber', 'wor', 'har', 'pre', 'ial', 'nat', 'act', 'rea', 'ali', 'us_', 'af', 'ft', 'rk', 'cu', 'ok', 'ya', 'aw', 'qu', 'ds', 'he_', 'ina', 'na_', 'lin', 'des', 'fer', 'pla', 'chi', 'ss_', 'orm', 'cti', 'ned', 'med', 'sio', 'igh', 'era', 'sl', 'go', 'oa', 'ini', 'shi', 'und', 'ari', 'tar', 'ell', 'rec', 'ia_', 'por', 'rie', 'oth', 'ani', 'ost', 'ity', 'ure', 'ans', 'ib', 'ug', 'ys', 'tan', 'hin', 'bli', 'lan', 'ies', 'app', 'tre', 'mer', 'rth', 'ntr', 'ach', 'rt_', 'ara', 'tes', 'ner', 'lle', 'eat', 'ass', 'din', 'ise', 'ny', 'ht', 'fu', 'og', 'hr', 'ew', 'tiv', 'dis', 'che', 'ast', 'rn_', 'tal', 'ger', 'lay', 'ele', 'ard', 'rd_', 'ven', 'ong', 'ded', 'ssi', 'ral', 'ris', 'nsi', 'sto', 'nu', 'ui', 'wn', 'pt', 'ph', 'tl', 'pu', 'nis', 'tri', 'ire', 'one', 'to_', 'uni', 'mar', 'ten', 'een', 'lon', 'ont', 'cia', 'own', 'ade', 'arr', 'ori', 'omp', 'lar', 'dia', 'oll', 'kha', 'nk', 'ks', 'sm', 'ms', 'gn', 'tw', 'ey', 'eo', 'les', 'cou', 'uth', 'ack', 'ad_', 'amp', 'ut_', 'sid', 'ses', 'led', 'fic', 'ase', 'abl', 'oi', 'xt', 'bh', 'mat', 'ref', 'ham', 'sea', 'ks_', 'gre', 'ser', 'elo', 'nge', 'tho', 'isi', 'cat', 'eme', 'cen', 'ds_', 'rem', 'ema', 'sti', 'tat', 'omi', 'ma_', 'ble', 'fl', 'gl', 'hy', 'hu', 'lm', 'lla', 'ict', 'unt', 'it_', 'wes', 'efe', 'ann', 'emb', 'mbe', 'rma', 'cor', 'ls_', 'son', 'om_', 'ved', 'nit', 'nne', 'win', 'ext', 'et_', 'eng', 'ave', 'cam', 'ir_', 'ndi', 'his', 'esi', 'den', 'ian', 'hil', 'ght', 'ice', 'ula', 'use', 'cte', 'eb', 'aj', 'ith', 'tel', 'ita', 'lli', 'orn', 'als', 'ue_', 'rom', 'rac', 'rel', 'arc', 'rge', 'rna', 'ord', 'ond', 'ory', 'ook', 'rti', 'sit', 'car', 'am_', 'tro', 'ya_', 'ang', 'vis', 'ual', 'ead', 'ey_', 'inc', 'nts', 'la_', 'rou', 'ugh', 'rne', 'she', 'war', 'ee_', 'ors', 'mpa', 'tte', 'ram', 'hou', 'ros', 'hes', 'omm', 'gin', 'vo', 'lf', 'oh', 'hn', 'u_', 'ik', 'ek', 'ka_', 'sou', 'hea', 'nag', 'ore', 'rop', 'bar', 'sec', 'hat', 'ars', 'spo', 'san', 'nta', 'ck_', 'ins', 'ime', 'ora', 'ild', 'att', 'fin', 'mai', 'ow_', 'ivi', 'ys_', 'ama', 'off', 'ici', 'lic', 'ens', 'sal', 'nch', 'ms_', 'mil', 'ert', 'sho', 'eu', 'rv', 'eq', 'az', 'ps', 'rf', 'dh', 'ob', 'ubl', 'ip_', 'dec', 'ein', 'oot', 'aye', 'pea', 'mes', 'fte', 'ary', 'nor', 'fra', 'ope', 'orl', 'wel', 'tha', 'lat', 'spe', 'vin', 'esp', 'rte', 'pan', 'equ', 'clu', 'uri', 'el_', 'ose', 'ier', 'not', 'vel', 'bac', 'iss', 'gh_', 'ile', 'urn', 'tim', 'col', 'sen', 'hig', 'cri', 'kin', 'gra', 'eli', 'low', 'cla', 'alt', 'acc', 'hah', 'cto', 'ero', 'air', 'ash', 'sse', 'yi', 'nl', 'uk', 'gs', 'dd', 'of_', 'ny_', 'wit', 'thi', 'esh', 'hip', 'ely', 'bor', 'whi', 'ich', 'cer', 'urt', 'il_', 'ngl', 'ake', 'ke_', 'ta_', 'cho', 'hoo', 'wn_', 'tem', 'any', 'rme', 'nst', 'ra_', 'que', 'ppo', 'oin', 'hed', 'sur', 'arl', 'ign', 'pas', 'ncl', 'ath', 'pri', 'thr', 'oug', 'eal', 'ail', 'bri', 'oli', 'dre', 'ffe', 'nam', 'ied', 'rai', 'ffi', 'ngs', 'tis', 'ood', 'ndo', 'edi', 'mme', 'adi', 'fil', 'hi_', 'erf', 'rep', 'sis', 'b_', 'kl', 'bs', 'ju', 'xp', 'ix', 'kn', 'wr', 'vil', 'pol', 'lie', 'ima', 'reg', 'li_', 'ret', 'boo', 'cce', 'gue', 'ht_', 'sch', 'xte', 'rce', 'cel', 'has', 'non', 'irs', 'ol_', 'val', 'len', 'mal', 'ien', 'suc', 'but', 'dra', 'tua', 'exp', 'fou', 'lud', 'ude', 'ami', 'tie', 'nto', 'owe', 'ace', 'now', 'hos', 'gen', 'ese', 'gro', 'ple', 'rov', 'duc', 'ene', 'nes', 'pub', 'atu', 'tle', 'rni', 'abo', 'evi', 'mot', 'lio', 'rap', 'arn', 'oss', 'pic', 'rfo', 'kar', 'usi', 'za', 'sb', 'by', 'ws', 'yo', 'x_', 'z_', 'iz', 'ze', 'oy', 'tc', 'hm', 'nf', 'ola', 'ece', 'eti', 'erm', 'rre', 'nov', 'ppe', 'aft', 'mor', 'yin', 'ork', 'rke', 'ked', 'erg', 'mir', 'rch', 'bra', 'hon', 'ono', 'nou', 'ean', 'up_', 'rld', 'was', 'erv', 'by_', 'mit', 'ok_', 'yea', 'oul', 'uld', 'bec', 'mpi', 'pio', 'whe', 'spa', 'ale', 'llo', 'rio', 'die', 'cro', 'hom', 'oma', 'phy', 'aga', 'hel', 'sub', 'uen', 'ria', 'old', 'two', 'ote', 'pon', 'ura', 'rm_', 'fai', 'bol', 'ew_', 'cit', 'wer', 'los', 'arm', 'cco', 'tru', 'tia', 'ets', 'ced', 'pat', 'hav', 'gs_', 'bas', 'ctu', 'ovi', 'hor', 'ghe', 'ps_', 'las', 'ot_', 'rev', 'tab', 'cre', 'ban', 'oni', 'udi', 'ah_', 'ilm', 'far', 'awa', 'ri_', 'ett', 'asp', 'lai', 'rsh', 'ni_', 'aff', 'ron', 'stu', 'bil', 'bha', 'ane', 'ko', 'bb', 'zi', 'sw', 'ank', 'ct_', 'egi', 'foo', 'bal', 'err', 'lig', 'urg', 'gam', 'can', 'fro', 'ayi', 'ega', 'mov', 'rab', 'ab_', 'ira', 'ged', 'urs', 'inn', 'ink', 'uar', 'nse', 'bea', 'pel', 'mis', 'rvi']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gwv02S2mFzXg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}